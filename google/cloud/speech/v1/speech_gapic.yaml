type: com.google.api.codegen.ConfigProto
config_schema_version: 1.0.0
language_settings:
  java:
    package_name: com.google.cloud.speech.v1
  python:
    package_name: google.cloud.speech_v1.gapic
  go:
    package_name: cloud.google.com/go/speech/apiv1
    release_level: GA
  csharp:
    package_name: Google.Cloud.Speech.V1
    release_level: GA
  ruby:
    package_name: Google::Cloud::Speech::V1
  php:
    package_name: Google\Cloud\Speech\V1
  nodejs:
    package_name: speech.v1
    domain_layer_location: google-cloud
interfaces:
- name: google.cloud.speech.v1.Speech
  smoke_test:
    method: Recognize
    init_fields:
    - config.language_code="en-US"
    - config.sample_rate_hertz=44100
    - config.encoding=FLAC
    - audio.uri="gs://gapic-toolkit/hello.flac"
  collections: []
  retry_codes_def:
  - name: idempotent
    retry_codes:
    - UNAVAILABLE
    - DEADLINE_EXCEEDED
  - name: non_idempotent
    retry_codes: []
  retry_params_def:
  - name: default
    initial_retry_delay_millis: 100
    retry_delay_multiplier: 1.3
    max_retry_delay_millis: 60000
    initial_rpc_timeout_millis: 1000000
    rpc_timeout_multiplier: 1
    max_rpc_timeout_millis: 1000000
    total_timeout_millis: 5000000
  methods:
  - name: Recognize
    flattening:
      groups:
      - parameters:
        - config
        - audio
    required_fields:
    - config
    - audio
    sample_code_init_fields:
    - config.encoding=FLAC
    - config.sample_rate_hertz=44100
    - config.language_code="en-US"
    - audio.uri=gs://bucket_name/file_name.flac
    retry_codes_name: idempotent
    retry_params_name: default
    timeout_millis: 1000000
    samples:
      standalone:
      - region_tag: speech_transcribe_sync_gcs
        value_sets:
        - speech_transcribe_sync_gcs
      - region_tag: speech_transcribe_sync
        value_sets:
        - speech_transcribe_sync
      - region_tag: speech_transcribe_multichannel
        value_sets:
        - speech_transcribe_multichannel
      - region_tag: speech_transcribe_model_selection_gcs
        value_sets:
        - speech_transcribe_model_selection_gcs
      - region_tag: speech_transcribe_async_word_time_offsets_gcs
        value_sets:
        - speech_transcribe_async_word_time_offsets_gcs
      - region_tag: speech_transcribe_model_selection
        value_sets:
        - speech_transcribe_model_selection
      - region_tag: speech_transcribe_multichannel_gcs
        value_sets:
        - speech_transcribe_multichannel_gcs
      - region_tag: speech_transcribe_enhanced_model
        value_sets:
        - speech_transcribe_enhanced_model
    sample_value_sets:
    - id: speech_transcribe_model_selection_gcs
      title: Selecting a Transcription Model (Cloud Storage)
      description: 'Transcribe a short audio file from Cloud Storage using a specified
        transcription model

'
      parameters:
        defaults:
        - audio.uri = "gs://cloud-samples-data/speech/hello.wav"
        - config.model = "phone_call"
        - config.language_code = "en-US"
        attributes:
        - parameter: audio.uri
          sample_argument_name: storage_uri
          description: URI for audio file in Cloud Storage, e.g. gs://[BUCKET]/[FILE]
        - parameter: config.model
          sample_argument_name: model
          description: |
            The transcription model to use, e.g. video, phone_call, default
            For a list of available transcription models, see:
            https://cloud.google.com/speech-to-text/docs/transcription-model#transcription_models
        - parameter: config.language_code
          description: The language of the supplied audio
      on_success:
      - loop:
          variable: result
          collection: "$resp.results"
          body:
          - comment:
            - First alternative is the most probable result
          - define: alternative=result.alternatives[0]
          - print:
            - 'Transcript: %s'
            - alternative.transcript
    - id: speech_transcribe_sync_gcs
      title: Transcript Audio File (Cloud Storage)
      description: 'Transcribe short audio file from Cloud Storage using synchronous
        speech recognition

'
      parameters:
        defaults:
        - audio.uri = "gs://cloud-samples-data/speech/brooklyn_bridge.raw"
        - config.sample_rate_hertz = 16000
        - config.language_code = "en-US"
        - config.encoding = LINEAR16
        attributes:
        - parameter: audio.uri
          sample_argument_name: storage_uri
          description: URI for audio file in Cloud Storage, e.g. gs://[BUCKET]/[FILE]
        - parameter: config.language_code
          description: The language of the supplied audio
        - parameter: config.sample_rate_hertz
          description: Sample rate in Hertz of the audio data sent
        - parameter: config.encoding
          description: |
            Encoding of audio data sent. This sample sets this explicitly.
            This field is optional for FLAC and WAV audio formats.
      on_success:
      - loop:
          variable: result
          collection: "$resp.results"
          body:
          - comment:
            - First alternative is the most probable result
          - define: alternative=result.alternatives[0]
          - print:
            - 'Transcript: %s'
            - alternative.transcript
    - id: speech_transcribe_sync
      title: Transcribe Audio File (Local File)
      description: Transcribe a short audio file using synchronous speech recognition
      parameters:
        defaults:
        - audio.content = "resources/brooklyn_bridge.raw"
        - config.language_code = "en-US"
        - config.sample_rate_hertz = 16000
        - config.encoding = LINEAR16
        attributes:
        - parameter: audio.content
          sample_argument_name: local_file_path
          read_file: true
          description: Path to local audio file, e.g. /path/audio.wav
        - parameter: config.language_code
          description: The language of the supplied audio
        - parameter: config.sample_rate_hertz
          description: Sample rate in Hertz of the audio data sent
        - parameter: config.encoding
          description: |
            Encoding of audio data sent. This sample sets this explicitly.
            This field is optional for FLAC and WAV audio formats.
      on_success:
      - loop:
          variable: result
          collection: "$resp.results"
          body:
          - comment:
            - First alternative is the most probable result
          - define: alternative=result.alternatives[0]
          - print:
            - 'Transcript: %s'
            - alternative.transcript
    - id: speech_transcribe_model_selection
      title: Selecting a Transcription Model (Local File)
      description: Transcribe a short audio file using a specified transcription model
      parameters:
        defaults:
        - audio.content = "resources/hello.wav"
        - config.model = "phone_call"
        - config.language_code = "en-US"
        attributes:
        - parameter: audio.content
          sample_argument_name: local_file_path
          read_file: true
          description: Path to local audio file, e.g. /path/audio.wav
        - parameter: config.model
          sample_argument_name: model
          description: |
            The transcription model to use, e.g. video, phone_call, default
            For a list of available transcription models, see:
            https://cloud.google.com/speech-to-text/docs/transcription-model#transcription_models
        - parameter: config.language_code
          description: The language of the supplied audio
      on_success:
      - loop:
          variable: result
          collection: "$resp.results"
          body:
          - comment:
            - First alternative is the most probable result
          - define: alternative=result.alternatives[0]
          - print:
            - 'Transcript: %s'
            - alternative.transcript
    - id: speech_transcribe_multichannel_gcs
      title: Multi-Channel Audio Transcription (Cloud Storage)
      description: 'Transcribe a short audio file from Cloud Storage with multiple
        channels

'
      parameters:
        defaults:
        - audio.uri = "gs://cloud-samples-data/speech/multi.wav"
        - config.audio_channel_count = 2
        - config.enable_separate_recognition_per_channel = True
        - config.language_code = "en-US"
        attributes:
        - parameter: audio.uri
          sample_argument_name: storage_uri
          description: URI for audio file in Cloud Storage, e.g. gs://[BUCKET]/[FILE]
        - parameter: config.audio_channel_count
          description: The number of channels in the input audio file (optional)
        - parameter: config.enable_separate_recognition_per_channel
          description: |
            When set to true, each audio channel will be recognized separately.
            The recognition result will contain a channel_tag field to state which
            channel that result belongs to
        - parameter: config.language_code
          description: The language of the supplied audio
      on_success:
      - loop:
          variable: result
          collection: "$resp.results"
          body:
          - comment:
            - "%s to recognize which audio channel this result is for"
            - channel_tag
          - print:
            - 'Channel tag: %s'
            - result.channel_tag
          - comment:
            - First alternative is the most probable result
          - define: alternative=result.alternatives[0]
          - print:
            - 'Transcript: %s'
            - alternative.transcript
    - id: speech_transcribe_multichannel
      title: Multi-Channel Audio Transcription (Local File)
      description: Transcribe a short audio file with multiple channels
      parameters:
        defaults:
        - audio.content = "resources/multi.wav"
        - config.audio_channel_count = 2
        - config.enable_separate_recognition_per_channel = True
        - config.language_code = "en-US"
        attributes:
        - parameter: audio.content
          sample_argument_name: local_file_path
          read_file: true
          description: Path to local audio file, e.g. /path/audio.wav
        - parameter: config.audio_channel_count
          description: The number of channels in the input audio file (optional)
        - parameter: config.enable_separate_recognition_per_channel
          description: |
            When set to true, each audio channel will be recognized separately.
            The recognition result will contain a channel_tag field to state which
            channel that result belongs to
        - parameter: config.language_code
          description: The language of the supplied audio
      on_success:
      - loop:
          variable: result
          collection: "$resp.results"
          body:
          - comment:
            - "%s to recognize which audio channel this result is for"
            - channel_tag
          - print:
            - 'Channel tag: %s'
            - result.channel_tag
          - comment:
            - First alternative is the most probable result
          - define: alternative=result.alternatives[0]
          - print:
            - 'Transcript: %s'
            - alternative.transcript
    - id: speech_transcribe_enhanced_model
      title: Using Enhanced Models (Local File)
      description: Transcribe a short audio file using an enhanced model
      parameters:
        defaults:
        - audio.content = "resources/hello.wav"
        - config.model = "phone_call"
        - config.use_enhanced = True
        - config.language_code = "en-US"
        attributes:
        - parameter: audio.content
          sample_argument_name: local_file_path
          read_file: true
          description: Path to local audio file, e.g. /path/audio.wav
        - parameter: config.model
          description: |
            The enhanced model to use, e.g. phone_call
            Currently phone_call is the only model available as an enhanced model.
        - parameter: config.use_enhanced
          description: |
            Use an enhanced model for speech recognition (when set to true).
            Project must be eligible for requesting enhanced models.
            Enhanced speech models require that you opt-in to data logging.
        - parameter: config.language_code
          description: The language of the supplied audio
      on_success:
      - loop:
          variable: result
          collection: "$resp.results"
          body:
          - comment:
            - First alternative is the most probable result
          - define: alternative=result.alternatives[0]
          - print:
            - 'Transcript: %s'
            - alternative.transcript
  - name: LongRunningRecognize
    flattening:
      groups:
      - parameters:
        - config
        - audio
    required_fields:
    - config
    - audio
    sample_code_init_fields:
    - config.encoding=FLAC
    - config.sample_rate_hertz=44100
    - config.language_code="en-US"
    - audio.uri=gs://bucket_name/file_name.flac
    retry_codes_name: non_idempotent
    retry_params_name: default
    timeout_millis: 60000
    long_running:
      return_type: google.cloud.speech.v1.LongRunningRecognizeResponse
      metadata_type: google.cloud.speech.v1.LongRunningRecognizeMetadata
      initial_poll_delay_millis: 20000
      poll_delay_multiplier: 1.5
      max_poll_delay_millis: 45000
      total_poll_timeout_millis: 86400000
    samples:
      standalone:
      - region_tag: speech_transcribe_async_gcs
        value_sets:
        - speech_transcribe_async_gcs
      - region_tag: speech_transcribe_async
        value_sets:
        - speech_transcribe_async
      - region_tag: speech_transcribe_async_word_time_offsets_gcs
        value_sets:
        - speech_transcribe_async_word_time_offsets_gcs
    sample_value_sets:
    - id: speech_transcribe_async_gcs
      title: Transcript Audio File using Long Running Operation (Cloud Storage) (LRO)
      description: 'Transcribe long audio file from Cloud Storage using asynchronous
        speech recognition

'
      parameters:
        defaults:
        - audio.uri = "gs://cloud-samples-data/speech/brooklyn_bridge.raw"
        - config.sample_rate_hertz = 16000
        - config.language_code = "en-US"
        - config.encoding = LINEAR16
        attributes:
        - parameter: audio.uri
          sample_argument_name: storage_uri
          description: URI for audio file in Cloud Storage, e.g. gs://[BUCKET]/[FILE]
        - parameter: config.language_code
          description: The language of the supplied audio
        - parameter: config.sample_rate_hertz
          description: Sample rate in Hertz of the audio data sent
        - parameter: config.encoding
          description: |
            Encoding of audio data sent. This sample sets this explicitly.
            This field is optional for FLAC and WAV audio formats.
      on_success:
      - loop:
          variable: result
          collection: "$resp.results"
          body:
          - comment:
            - First alternative is the most probable result
          - define: alternative=result.alternatives[0]
          - print:
            - 'Transcript: %s'
            - alternative.transcript
    - id: speech_transcribe_async
      title: Transcribe Audio File using Long Running Operation (Local File) (LRO)
      description: Transcribe a long audio file using asynchronous speech recognition
      parameters:
        defaults:
        - audio.content = "resources/brooklyn_bridge.raw"
        - config.language_code = "en-US"
        - config.sample_rate_hertz = 16000
        - config.encoding = LINEAR16
        attributes:
        - parameter: audio.content
          sample_argument_name: local_file_path
          read_file: true
          description: Path to local audio file, e.g. /path/audio.wav
        - parameter: config.language_code
          description: The language of the supplied audio
        - parameter: config.sample_rate_hertz
          description: Sample rate in Hertz of the audio data sent
        - parameter: config.encoding
          description: |
            Encoding of audio data sent. This sample sets this explicitly.
            This field is optional for FLAC and WAV audio formats.
      on_success:
      - loop:
          variable: result
          collection: "$resp.results"
          body:
          - comment:
            - First alternative is the most probable result
          - define: alternative=result.alternatives[0]
          - print:
            - 'Transcript: %s'
            - alternative.transcript
    - id: speech_transcribe_async_word_time_offsets_gcs
      title: Getting word timestamps (Cloud Storage) (LRO)
      description: 'Print start and end time of each word spoken in audio file from
        Cloud Storage

'
      parameters:
        defaults:
        - audio.uri = "gs://cloud-samples-data/speech/brooklyn_bridge.flac"
        - config.enable_word_time_offsets = True
        - config.language_code = "en-US"
        attributes:
        - parameter: audio.uri
          sample_argument_name: storage_uri
          description: URI for audio file in Cloud Storage, e.g. gs://[BUCKET]/[FILE]
        - parameter: config.enable_word_time_offsets
          description: |
            When enabled, the first result returned by the API will include a list
            of words and the start and end time offsets (timestamps) for those words.
        - parameter: config.language_code
          description: The language of the supplied audio
      on_success:
      - comment:
        - The first result includes start and end time word offsets
      - define: result=$resp.results[0]
      - comment:
        - First alternative is the most probable result
      - define: alternative=result.alternatives[0]
      - print:
        - 'Transcript: %s'
        - alternative.transcript
      - comment:
        - Print the start and end time of each word
      - loop:
          collection: alternative.words
          variable: word
          body:
          - print:
            - 'Word: %s'
            - word.word
          - print:
            - 'Start time: %s seconds %s nanos'
            - word.start_time.seconds
            - word.start_time.nanos
          - print:
            - 'End time: %s seconds %s nanos'
            - word.end_time.seconds
            - word.end_time.nanos
  - name: StreamingRecognize
    retry_codes_name: idempotent
    retry_params_name: default
    timeout_millis: 1000000
